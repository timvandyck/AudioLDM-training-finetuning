# ========================================================================
# FINALE, VERIFIZIERTE YAML für das Fine-Tuning von AudioLDM 2
# Kompatibel mit latent_diffusion.py und deiner Ordnerstruktur
# ========================================================================

# --- Top-Level Pfade & Projektinfos ---
# Basisverzeichnis für alle Logs und Checkpoints
log_directory: "./experiments/" 
project: "audioldm2-finetune-elephants"
seed: 42
precision: "high"

# --- Laden des vortrainierten Modells ---
# Das Skript nutzt diesen Schlüssel, um die Gewichte zu laden, BEVOR das Training beginnt.
# Gib hier den Pfad zu deiner heruntergeladenen audioldm2-full.pth an.
reload_from_ckpt: "./data/checkpoints/audioldm2-full.pth"

# --- Datenkonfiguration (entscheidend!) ---
data:
  # Das Skript erwartet eine Liste von Namen für das Training...
  train: ["elephants_train"]
  # ...und einen einzelnen Namen für die Validierung
  val: "elephants_val"
  # Diese JSON-Datei ist der "Wegweiser" für die AudioDataset-Klasse.
  metadata_root: "/data/dataset/metadata/elephants_metadata.json"
  # Das ist der Basispfad, von dem aus die Pfade in den CSVs gelesen werden.
  data_root: "/Users/yvdalt/Dropbox/FH_Stp/Semester_2/Elecom/Audio LDM/Audio LDM 1 - tSNE/elephant_data/audio_chunks"
  # Die Spalte in deiner CSV, die den relativen Pfad zur Audiodatei enthält.
  audio_key: "file_path" 
  # Die Spalte, die die Altersklasse (unser Text-Prompt) enthält.
  caption_key: "age_class"

# --- Trainingsschritte & Speicher-Konfiguration ---
step:
  validation_every_n_epochs: 5
  save_checkpoint_every_n_steps: 500
  max_steps: 50000
  save_top_k: 3

# --- Preprocessing (angepasst für AudioMAE) ---
preprocessing:
  audio:
    sampling_rate: 16000
    duration: 10.24
  stft:
    filter_length: 1024
    hop_length: 160
    win_length: 1024
  mel:
    # WICHTIG: AudioMAE und der VAE von AudioLDM 2 arbeiten mit 128 Mel-Bins
    n_mel_channels: 128
    mel_fmin: 0
    mel_fmax: 8000

# --- Modellarchitektur (definiert AudioLDM 2) ---
model:
  target: audioldm_train.model.latent_diffusion.LatentDiffusion
  params:
    base_learning_rate: 1.0e-5
    batchsize: 4 # Klein anfangen, kann bei Bedarf erhöht werden
    
    # Pfad zu den AudioMAE-Gewichten
    audiomae_path: "./pretrained_models/audiomae-full-base.pth"

    # VAE Konfiguration (First Stage)
    first_stage_config:
      target: audioldm_train.model.autoencoder.AutoencoderKL
      params:
        embed_dim: 8
        lossconfig: { target: torch.nn.Identity }
        ddconfig:
          double_z: True
          z_channels: 8
          resolution: 256
          in_channels: 1
          out_ch: 1
          ch: 128
          ch_mult: [1, 2, 4]
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0

    # T-UNet Konfiguration
    unet_config:
      target: audioldm_train.model.modules.diffusionmodules.openaimodel.UNetModel
      params:
        in_channels: 8
        out_channels: 8
        model_channels: 256
        attention_resolutions: [8, 4, 2]
        num_res_blocks: 2
        channel_mult: [1, 2, 3, 5]
        num_head_channels: 64
        use_spatial_transformer: True
        transformer_depth: 1
        n_trans: 2
        context_dim: 768

    # Conditioning Stage (GPT-2 etc.)
    cond_stage_config:
      target: audioldm_train.model.modules.encoders.modules.GeneralConditioner
      params:
        emb_models:
          - target: audioldm_train.model.modules.encoders.modules.CLAPEmbeddingProvider
            params:
              clap_path: "./pretrained_models/clap_htsat_tiny.pt"
              embedding_dim: 1024
          - target: audioldm_train.model.modules.encoders.modules.T5FilmEmbeddingProvider
            params:
              t5_path: "google/flan-t5-large"
              embedding_dim: 1024